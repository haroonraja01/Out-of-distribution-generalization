{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1563cef-bed8-4a95-b627-5929571e97c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "#import time\n",
    "from logging import getLogger\n",
    "#import warnings\n",
    "\n",
    "import numpy as np \n",
    "#from tqdm import tqdm\n",
    "import yaml \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.parallel\n",
    "#import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data as data\n",
    "import torch.distributed as dist\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167aa0ef-bdbc-49b6-a33c-b4954d674f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "441ec94d-90d4-47c4-a832-319cda5febf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "\tbool_flag,\n",
    "\tinitialize_exp,\n",
    "\trestart_from_checkpoint,\n",
    "\tfix_random_seeds,\n",
    "\tAverageMeter,\n",
    "\tinit_distributed_mode,\n",
    "\taccuracy,\n",
    "\tadd_slurm_params,\n",
    "\tget_dataloader,\n",
    "\toptimizer_config,\n",
    ")\n",
    "\n",
    "from models import get_model, get_classifier, modelfusion\n",
    "from datasets import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae33a9e1-52dd-45d1-b0db-a051b807c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, reglog, optimizer, loader,  epoch, args ):\n",
    "\t\"\"\"\n",
    "\tTrain the models on the dataset.\n",
    "\t\"\"\"\n",
    "\t# running statistics\n",
    "\tbatch_time = AverageMeter()\n",
    "\tdata_time = AverageMeter()\n",
    "\n",
    "\t# training statistics\n",
    "\ttop1 = AverageMeter()\n",
    "\ttop5 = AverageMeter()\n",
    "\tlosses = AverageMeter()\n",
    "\tend = time.perf_counter()\n",
    "\t\n",
    "\tmodel.eval()\n",
    "\treglog.train()\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "\tfor iter_epoch, record in enumerate(loader):\n",
    "\t\t# measure data loading time\n",
    "\t\tdata_time.update(time.perf_counter() - end)\n",
    "\t\t\n",
    "\n",
    "\t\tif len(record) == 2:\n",
    "\t\t\tinp, target = record \n",
    "\t\telif len(record) == 3:\n",
    "\t\t\n",
    "\t\t\tinp, target, meta = record \n",
    "\t\t\n",
    "\t\t#move to gpu\n",
    "        # commenting next two lines for running on a cpu\n",
    "\t\t#inp = inp.cuda(non_blocking=True)\n",
    "\t\t#target = target.cuda(non_blocking=True)\n",
    "\t\t# forward\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutput = model(inp)\n",
    "\n",
    "\t\toutput = reglog(output)\n",
    "\t\tloss = criterion(output, target) \n",
    "\n",
    "\t\t# compute the gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# update stats\n",
    "\t\tacc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\t\tlosses.update(loss.item(), inp.size(0))\n",
    "\t\ttop1.update(acc1[0], inp.size(0))\n",
    "\t\ttop5.update(acc5[0], inp.size(0))\n",
    "\n",
    "\t\tbatch_time.update(time.perf_counter() - end)\n",
    "\t\tend = time.perf_counter()\n",
    "\n",
    "\t\t# verbose\n",
    "\t\tif args.rank == 0 and iter_epoch % 50 == 0:\n",
    "\t\t\t\n",
    "\t\t\tlogger.info(\n",
    "\t\t\t\t\"Epoch[{0}] - Iter: [{1}/{2}]\\t\"\n",
    "\t\t\t\t\"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "\t\t\t\t\"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "\t\t\t\t\"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
    "\t\t\t\t\"Prec {top1.val:.3f} ({top1.avg:.3f})\\t\"\n",
    "\t\t\t\t\"LR {lr}\".format(\n",
    "\t\t\t\t\tepoch,\n",
    "\t\t\t\t\titer_epoch,\n",
    "\t\t\t\t\tlen(loader),\n",
    "\t\t\t\t\tbatch_time=batch_time,\n",
    "\t\t\t\t\tdata_time=data_time,\n",
    "\t\t\t\t\tloss=losses,\n",
    "\t\t\t\t\ttop1=top1,\n",
    "\t\t\t\t\tlr=optimizer.param_groups[0][\"lr\"],\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\n",
    "\treturn epoch, losses.avg, top1.avg.item(), top5.avg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64e5677c-9dc5-466b-85fa-476ab4f90b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_network(val_loader, model, classifier, args, indices=None):\n",
    "\tbatch_time = AverageMeter()\n",
    "\tlosses = AverageMeter()\n",
    "\ttop1 = AverageMeter()\n",
    "\ttop5 = AverageMeter()\n",
    "\t#global best_acc\n",
    "\n",
    "\t# switch to evaluate mode\n",
    "\tmodel.eval()\n",
    "\tclassifier.eval()\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss().cuda()\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tend = time.perf_counter()\n",
    "\t\tfor i, record in enumerate(val_loader):\n",
    "\t\t\tif len(record) == 2:\n",
    "\t\t\t\tinp, target = record \n",
    "\t\t\telif len(record) == 3:\n",
    "\t\t\t\tinp, target, meta = record \n",
    "\t\t\t\n",
    "\t\t\t# move to gpu\n",
    "\t\t\t#inp = inp.cuda(non_blocking=True)\n",
    "\t\t\t#target = target.cuda(non_blocking=True)\n",
    "\n",
    "\t\t\t# compute output\n",
    "\t\t\toutput = classifier(model(inp))\n",
    "\t\t\t\n",
    "\t\n",
    "\n",
    "\n",
    "\t\t\tif indices is not None:\n",
    "\t\t\t\toutput = output[:,indices]\n",
    "\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\n",
    "\t\t\tacc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "\t\t\tlosses.update(loss.item(), inp.size(0))\n",
    "\n",
    "\t\t\ttop1.update(acc1[0], inp.size(0))\n",
    "\t\t\ttop5.update(acc5[0], inp.size(0))\n",
    "\n",
    "\t\t\t# measure elapsed time\n",
    "\t\t\tbatch_time.update(time.perf_counter() - end)\n",
    "\t\t\tend = time.perf_counter()\n",
    "\t\t\tif args.rank == 0 and i % 50 == 0:\n",
    "\t\t\t\tlogger.info(\n",
    "\t\t\t\t\"Epoch[{0}] - Iter: [{1}/{2}]\\t\"\n",
    "\t\t\t\t\"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "\t\t\t\t\"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
    "\t\t\t\t\"Prec {top1.val:.3f} ({top1.avg:.3f})\\t\".format(\n",
    "\t\t\t\t\t0,\n",
    "\t\t\t\t\ti,\n",
    "\t\t\t\t\tlen(val_loader),\n",
    "\t\t\t\t\tbatch_time=batch_time,\n",
    "\t\t\t\t\tloss=losses,\n",
    "\t\t\t\t\ttop1=top1,\n",
    "\t\t\t\t)\n",
    "\t\t\t\t)\n",
    "\n",
    "\tscores_val = torch.Tensor(np.array([losses.sum, top1.sum.item(), top5.sum.item(), \\\n",
    "\t\t\t\t\t\t\t\tlosses.count, top1.count, top5.count])).to(target.get_device())\n",
    "\tdist.all_reduce(scores_val, op=dist.ReduceOp.SUM)\n",
    "\tscores_val = tuple((scores_val[:3] / scores_val[3:]).detach().cpu().numpy().tolist())\n",
    "\tlosses, top1, top5  = scores_val\n",
    "\treturn losses, top1, top5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa6e39b2-ceb2-4d62-b53f-5f509311112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "\tglobal best_acc\n",
    "\n",
    "\ttags = yaml.load(open('./configs/pretrained_checkpoints.yaml'), Loader=yaml.FullLoader)\n",
    "\n",
    "\tif args.tag is not None and args.tag in tags:\n",
    "\t\tfor key in tags[args.tag]:\n",
    "\t\t\tprint(key)\n",
    "\t\t\tsetattr(args, key, tags[args.tag][key])\n",
    "\n",
    "\t# distributed training environments and seeds\n",
    "\t#init_distributed_mode(args)\n",
    "\tfix_random_seeds(args.seed)\n",
    "\t\n",
    "\t# amd gpu cards environment variables\n",
    "\t# os.environ['MIOPEN_USER_DB_PATH']=os.path.join(args.dump_path, 'amd/rank_%d' % args.rank)\n",
    "\t# os.environ['MIOPEN_FIND_MODE']='2'\n",
    "\n",
    "\t# initialize logger ... \n",
    "\tlogger, training_stats = initialize_exp( args, \"epoch\", \"loss\", \"prec1\", \"prec5\", \"loss_val\", \"prec1_val\", \"prec5_val\")\n",
    "\n",
    "\t# build data\n",
    "\ttrain_dataset, val_dataset, datamsg = get_dataset(args.data_name, args.tf_name, args)\n",
    "\n",
    "\t# build dataloaders \n",
    "\ttrain_loader, val_loader, additional_loaders = get_dataloader(train_dataset, val_dataset, datamsg, args)\n",
    "\tlogger.info(\"Building data done\")\n",
    "\n",
    "\n",
    "\t## build trunk and load weights \n",
    "\ttotal_feat_dim, feat_dims, models = 0, [], []\n",
    "\n",
    "\tfor i in range(len(args.arch)):\n",
    "\t\n",
    "\t\tper_model, msg, feat_dim = get_model(args.arch[i], skip_pool=args.skip_pool, \\\n",
    "\t\tpretrain_path = None if len(args.pretrained)==0 else args.pretrained[i], img_dim=datamsg['img_dim'])\n",
    "\t\t#fix1st_pretrain_path = args.fix1st_pretrained ) #e.g. 'regnet_y_32gf'\n",
    "\t\tlogger.info(\"Load pretrained model with msg: {}\".format(msg))\n",
    "\n",
    "\t\tfeat_dims.append(feat_dim)\n",
    "\t\tmodels.append(per_model)\n",
    "\n",
    "\t\n",
    "\t#build classifier\n",
    "\tclassifier = get_classifier(args.classifier, datamsg['nclass'], feat_dims, logger, args)\n",
    "\n",
    "\t#print(classifier.linear.weight.data)\n",
    "\tlogger.info('classifier {}'.format(classifier))\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\t# model to gpu\n",
    "\t# device = torch.device(\"cuda:\" + str(args.gpu_to_work_on))\n",
    "    # Using cpu for now\n",
    "\n",
    "\t# model is either Identity or backbone. only classifier is trainable\n",
    "\tmodel, classifier = modelfusion(args.richway, models, classifier, args)\n",
    "\tmodel, classifier = model.to(device), classifier.to(device)\n",
    "\n",
    "    # For distributed usage in future\n",
    "\t# classifier = nn.parallel.DistributedDataParallel(\n",
    "\t# \tclassifier,\n",
    "\t# \tdevice_ids=[args.gpu_to_work_on],\n",
    "\t# )\n",
    "\n",
    "\toptimizer = optimizer_config(classifier, args, logger, \\\n",
    "\t\thead_reg = lambda x: True if args.exp_mode in ['lineareval','biaslineareval'] else lambda x: 'classifier' in x )\n",
    "\tlogger.info('optimizer {}'.format(optimizer))\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "\t# set scheduler\n",
    "\tif args.scheduler_type == \"step\":\n",
    "\t\tscheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "\t\t\toptimizer, args.decay_epochs, gamma=args.gamma\n",
    "\t\t)\n",
    "\telif args.scheduler_type == \"cosine\":\n",
    "\t\tscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "\t\t\toptimizer, args.epochs, eta_min=args.final_lr\n",
    "\t\t)\n",
    "\tlogger.info('lr scheduler {}'.format(scheduler))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# Optionally resume from a checkpoint\n",
    "\tto_restore = {\"epoch\": 0, \"best_acc\": 0.}\n",
    "\t\n",
    "\tif 'save' in args.mode:\n",
    "\t\trestart_from_checkpoint(\n",
    "\t\tos.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
    "\t\trun_variables=to_restore,\n",
    "\t\tstate_dict=classifier,\n",
    "\t\t)\n",
    "\t\tstart_epoch = to_restore[\"epoch\"]\n",
    "\t\tbest_acc = to_restore[\"best_acc\"]\n",
    "\telse:\n",
    "\t\trestart_from_checkpoint(\n",
    "\t\t\tos.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
    "\t\t\trun_variables=to_restore,\n",
    "\t\t\tstate_dict=classifier,\n",
    "\t\t\toptimizer=optimizer,\n",
    "\t\t\tscheduler=scheduler,\n",
    "\t\t)\n",
    "\t\tstart_epoch = to_restore[\"epoch\"]\n",
    "\t\tbest_acc = to_restore[\"best_acc\"]\n",
    "\n",
    "\t#cudnn.benchmark = True\n",
    "\teval('setattr(torch.backends.cudnn, \"benchmark\", True)')\n",
    "\t\n",
    "\tif args.cuda_deterministic:\n",
    "\t\tlogger.info(\"cuda deterministic\")\n",
    "\t\teval('setattr(torch.backends.cudnn, \"deterministic\", True)') \n",
    "\n",
    "\tfor epoch in range(start_epoch, args.epochs):\n",
    "\t\t\n",
    "\t\tif epoch == 0 and args.save_init:\n",
    "\t\t\tsave_dict = {\n",
    "\t\t\t\t\t\"epoch\": 0,\n",
    "\t\t\t\t\t\"state_dict\": classifier.state_dict(),\n",
    "\t\t\t\t\t\"optimizer\": optimizer.state_dict(),\n",
    "\t\t\t\t\t\"scheduler\": scheduler.state_dict(),\n",
    "\t\t\t\t\t\"best_acc\": 0,\n",
    "\t\t\t\t}\n",
    "\t\t\ttorch.save(save_dict, os.path.join(args.dump_path, f\"checkpoint_init.pth.tar\"))\n",
    "\t\t\tlogger.info('saved weight initialization')\n",
    "\t\t# train the network for one epoch\n",
    "\t\tlogger.info(\"============ Starting epoch %i ... ============\" % epoch)\n",
    "\n",
    "\t\t# set samplers\n",
    "\t\ttrain_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "\t\ttr_epoch, tr_loss, tr_top1, tr_top5 = train(model, classifier, optimizer, train_loader, epoch, args)\n",
    "\t\tscheduler.step()\n",
    "\n",
    "\t\tif (epoch+1) % args.eval_freq == 0: \n",
    "\t\t\tloss, top1, top5 = validate_network(val_loader, model,  classifier, args,)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tif args.custom_eval_func is not None: \n",
    "\t\t\t\tfrom src import custom_eval\n",
    "\t\t\t\tfor custom_eval_name in args.custom_eval_func:\n",
    "\t\t\t\t\tcustom_eval_func = getattr(custom_eval, custom_eval_name)\n",
    "\t\t\t\t\tcustom_eval_results = custom_eval_func(val_loader, model, classifier, args)\n",
    "\t\t\t\t\tlogger.info(f'{custom_eval_name}: ' + ','.join(['%.4f' % val for val in custom_eval_results]))\n",
    "\t\t\t\n",
    "\t\t\t# additional validation sets\n",
    "\t\t\tadditional_msg = {}\n",
    "\t\t\tfor key in additional_loaders:\n",
    "\t\t\t\tloader = additional_loaders[key]\n",
    "\t\t\t\tad_loss, ad_top1, ad_top5 = validate_network(loader, model,  classifier,args)\n",
    "\t\t\t\tadditional_msg[key]=[ad_loss, ad_top1, ad_top5]\n",
    "\n",
    "\t\t\ttraining_stats.update([tr_epoch, tr_loss, tr_top1, tr_top5] + [loss, top1, top5])\n",
    "\n",
    "\n",
    "\t\t\t# log best acc\n",
    "\t\t\t#global best_acc\n",
    "\t\t\tis_best = False \n",
    "\t\t\tif top1 > best_acc:\n",
    "\t\t\t\t#best_acc = top1.avg.item()\n",
    "\t\t\t\tbest_acc = top1\n",
    "\t\t\t\tis_best = True \n",
    "\n",
    "\t\t\tif args.rank == 0:\n",
    "\t\t\t\tlogger.info(\n",
    "\t\t\t\t\t\"Test:\\t\"\n",
    "\t\t\t\t\t\"Loss {loss:.4f}\\t\"\n",
    "\t\t\t\t\t\"Acc@1 {top1:.3f}\\t\"\n",
    "\t\t\t\t\t\"Best Acc@1 so far {acc:.1f}\".format(loss=loss, top1=top1, acc=best_acc))\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor key in additional_msg:\n",
    "\t\t\t\t\tloss, top1, _ = additional_msg[key]\n",
    "\t\t\t\t\tlogger.info(\n",
    "\t\t\t\t\t\t\"additional Test {key}:\\t\"\n",
    "\t\t\t\t\t\t\"Loss {loss:.4f}\\t\"\n",
    "\t\t\t\t\t\t\"Acc@1 {top1:.3f}\".format(key=key, loss=loss, top1=top1))\n",
    "\n",
    "\t\t\t# save checkpoint\n",
    "\t\t\tif args.rank == 0:\n",
    "\n",
    "\t\t\t\tsave_dict = {\n",
    "\t\t\t\t\t\"epoch\": epoch + 1,\n",
    "\t\t\t\t\t\"state_dict\": classifier.state_dict(),\n",
    "\t\t\t\t\t\"optimizer\": optimizer.state_dict(),\n",
    "\t\t\t\t\t\"scheduler\": scheduler.state_dict(),\n",
    "\t\t\t\t\t\"best_acc\": best_acc,\n",
    "\t\t\t\t}\n",
    "\t\t\t\ttorch.save(save_dict, os.path.join(args.dump_path, \"checkpoint.pth.tar\"))\n",
    "\t\t\t\t\n",
    "\t\t\t\tif (epoch+1) % args.save_freq == 0:\n",
    "\t\t\t\t\ttorch.save(save_dict, os.path.join(args.dump_path, f\"checkpoint_epoch{epoch+1}.pth.tar\"))\n",
    "\t\t\n",
    "\t\t\t\tif is_best:\n",
    "\t\t\t\t\ttorch.save(save_dict, os.path.join(args.dump_path, \"best.pth.tar\"))\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\tlogger.info(\"Training of the supervised linear classifier on frozen features completed.\\n\"\n",
    "\t\t\t\t\"Top-1 test accuracy: {acc:.1f}\".format(acc=best_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b5c6af-a333-4aa8-a9ef-bb1b4d88fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72df7141-3027-451a-a207-630a0b0366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_args(args):\n",
    "\tprint(args.tags, args.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "568b4678-ebcd-4852-a906-646c8231e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some 1\n"
     ]
    }
   ],
   "source": [
    "class Args(argparse.Namespace):\n",
    "    data = './data/penn'\n",
    "    model = 'LSTM'\n",
    "    emsize = 200\n",
    "    nhid = 200\n",
    "    tags = 'some'\n",
    "    epochs = 1\n",
    "    \n",
    "\n",
    "args=Args()\n",
    "test_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07550cf2-47c8-4142-b7a7-2b9401d4d688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
